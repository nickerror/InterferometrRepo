{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\"\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.epochs = 20\n",
    "        self.cuda=True\n",
    "        self.num_classes = 1\n",
    "        self.batch_size = 4\n",
    "        self.learning_rate = 0.01\n",
    "        self.train_size=0.8\n",
    "        self.dataset = \"InterferometerPhoto\"\n",
    "        #self.architecture = \"CNN\"\n",
    "        self.pin_memory = True\n",
    "        self.momentum = 0.9\n",
    "        self.step_size = 3\n",
    "        self.gamma = 0.1\n",
    "        self.dataset_metadata = \"../data/raw/1channel/reference/epsilon.csv\" # will change for processed\n",
    "        self.num_workers = 0\n",
    "        self.data_root_dir = \"../data/raw/1channel/photo\" # will change for processed\n",
    "        self.data_transforms = transforms.Compose([\n",
    "                #transforms.CenterCrop(448),\n",
    "                #transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.491, 0.491, 0.491],\n",
    "                                      std=[0.210, 0.210, 0.210]) \n",
    "                #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "class EpsilonDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(annotation_file,skiprows=0, delim_whitespace=' ')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = np.array(cv2.imread(os.path.join(self.root_dir, str(str(\"%05d\" %self.annotations.imgnr[index]))+ \".png\"))).astype(np.float32)\n",
    "        img=PIL.Image.fromarray(np.uint8(img))\n",
    "        y_label = self.annotations.eps[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, y_label\n",
    "\n",
    "def prepare_data(config):\n",
    "    dataset = EpsilonDataset(config.data_root_dir, config.dataset_metadata, transform=config.data_transforms)\n",
    "    \n",
    "    g = torch.Generator(device=device).manual_seed(0)\n",
    "    train_size = int(config.train_size * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size], generator=g)\n",
    "    print(\"len(train_dataset):\", len(train_dataset),\"len(val_dataset):\", len(val_dataset))\n",
    "    print(train_dataset)\n",
    "    loader_params = dict(batch_size=config.batch_size, num_workers=config.num_workers,\n",
    "                         pin_memory=config.pin_memory, generator=g, shuffle=True)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(**loader_params, dataset=train_dataset )\n",
    "    validation_loader = torch.utils.data.DataLoader(**loader_params, dataset=val_dataset )\n",
    "    #validation_loader = torch.utils.data.DataLoader(**loader_params, dataset=train_dataset )\n",
    "    return {'train': train_loader, 'val': validation_loader}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset): 1280 len(val_dataset): 320\n",
      "<torch.utils.data.dataset.Subset object at 0x0000021C36875A90>\n",
      "Device:  cpu\n",
      "Dataloader train len:  320 val len:  80\n"
     ]
    }
   ],
   "source": [
    "config=Config()\n",
    "\n",
    "dataloaders = prepare_data(config)\n",
    "dataset_sizes = {x: len(dataloaders[x]) for x in ['train', 'val']}\n",
    "#class_names = image_datasets['train'].classes\n",
    "dataset = EpsilonDataset(config.data_root_dir, config.dataset_metadata, transform=config.data_transforms)\n",
    "#print(dataset[0])\n",
    "train_features, train_labels=next(iter(dataloaders[\"train\"]))\n",
    "#print(f\"Feature batch shape: {train_features.size()}\")\n",
    "#print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "print(\"Device: \", device)\n",
    "print(\"Dataloader train len: \", len(dataloaders[\"train\"]), \"val len: \", len(dataloaders[\"val\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLossFunctionDebug(outputs, labels, totalLoss):\n",
    "    print(\"NextOne\")\n",
    "    for i in range (len(outputs)):\n",
    "        print(\"i: \", i, \"label: \", float(labels[i]), \"output:\", float(outputs[i]), \"diff= \", float(min( abs(abs(labels[i])-abs(outputs[i])) , abs(1-(abs(labels[i])-abs(outputs[i]))) )))#float(labels[i])-float(outputs[i]))\n",
    "\n",
    "    print(\"totalLoss:\", float(totalLoss))\n",
    "    return totalLoss\n",
    "\n",
    "\n",
    "def customLossFunction(outputs, labels):\n",
    "    totalLoss=0.0\n",
    "    for i in range (len(outputs)):\n",
    "        #oneOutputLoss=min(abs(abs(labels[i]%1-outputs[i]%1)), 1-(labels[i]%1-outputs[i]%1) )\n",
    "        oneOutputLoss=min( abs(abs(labels[i])-abs(outputs[i])) , abs(1-(abs(labels[i])-abs(outputs[i]))) )\n",
    "        totalLoss+=oneOutputLoss\n",
    "    totalLoss/=len(outputs)\n",
    "    #customLossFunctionDebug(outputs=outputs, labels=labels, totalLoss=totalLoss)\n",
    "    return totalLoss\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = -100000\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        #print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            #i=0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                #print(i, len(inputs))\n",
    "                #i=i+1\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss+=loss\n",
    "                #running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            #print(\"epoch_loss: \", epoch_loss, \"running_loss: \", running_loss, \"dataset_sizes[phase]: \", dataset_sizes[phase])\n",
    "            epoch_acc = 1-epoch_loss#running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                str(phase), float(epoch_loss), float(epoch_acc)))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(float(best_acc)))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/124\n",
      "train Loss: 0.4583 Acc: 0.5417\n",
      "val Loss: 0.2248 Acc: 0.7752\n",
      "\n",
      "Epoch 1/124\n",
      "train Loss: 0.3434 Acc: 0.6566\n",
      "val Loss: 0.1644 Acc: 0.8356\n",
      "\n",
      "Epoch 2/124\n",
      "train Loss: 0.2644 Acc: 0.7356\n",
      "val Loss: 0.2598 Acc: 0.7402\n",
      "\n",
      "Epoch 3/124\n",
      "train Loss: 0.2269 Acc: 0.7731\n",
      "val Loss: 0.3474 Acc: 0.6526\n",
      "\n",
      "Epoch 4/124\n",
      "train Loss: 0.2098 Acc: 0.7902\n",
      "val Loss: 0.1928 Acc: 0.8072\n",
      "\n",
      "Epoch 5/124\n",
      "train Loss: 0.2114 Acc: 0.7886\n",
      "val Loss: 0.1685 Acc: 0.8315\n",
      "\n",
      "Epoch 6/124\n",
      "train Loss: 0.1908 Acc: 0.8092\n",
      "val Loss: 0.1703 Acc: 0.8297\n",
      "\n",
      "Epoch 7/124\n",
      "train Loss: 0.1130 Acc: 0.8870\n",
      "val Loss: 0.0690 Acc: 0.9310\n",
      "\n",
      "Epoch 8/124\n",
      "train Loss: 0.1159 Acc: 0.8841\n",
      "val Loss: 0.2441 Acc: 0.7559\n",
      "\n",
      "Epoch 9/124\n",
      "train Loss: 0.0968 Acc: 0.9032\n",
      "val Loss: 0.0356 Acc: 0.9644\n",
      "\n",
      "Epoch 10/124\n",
      "train Loss: 0.0860 Acc: 0.9140\n",
      "val Loss: 0.0847 Acc: 0.9153\n",
      "\n",
      "Epoch 11/124\n",
      "train Loss: 0.0822 Acc: 0.9178\n",
      "val Loss: 0.0518 Acc: 0.9482\n",
      "\n",
      "Epoch 12/124\n",
      "train Loss: 0.0733 Acc: 0.9267\n",
      "val Loss: 0.0344 Acc: 0.9656\n",
      "\n",
      "Epoch 13/124\n",
      "train Loss: 0.0681 Acc: 0.9319\n",
      "val Loss: 0.1064 Acc: 0.8936\n",
      "\n",
      "Epoch 14/124\n",
      "train Loss: 0.0517 Acc: 0.9483\n",
      "val Loss: 0.0405 Acc: 0.9595\n",
      "\n",
      "Epoch 15/124\n",
      "train Loss: 0.0540 Acc: 0.9460\n",
      "val Loss: 0.0325 Acc: 0.9675\n",
      "\n",
      "Epoch 16/124\n",
      "train Loss: 0.0485 Acc: 0.9515\n",
      "val Loss: 0.0333 Acc: 0.9667\n",
      "\n",
      "Epoch 17/124\n",
      "train Loss: 0.0511 Acc: 0.9489\n",
      "val Loss: 0.0337 Acc: 0.9663\n",
      "\n",
      "Epoch 18/124\n",
      "train Loss: 0.0494 Acc: 0.9506\n",
      "val Loss: 0.0386 Acc: 0.9614\n",
      "\n",
      "Epoch 19/124\n",
      "train Loss: 0.0493 Acc: 0.9507\n",
      "val Loss: 0.0393 Acc: 0.9607\n",
      "\n",
      "Epoch 20/124\n",
      "train Loss: 0.0452 Acc: 0.9548\n",
      "val Loss: 0.0349 Acc: 0.9651\n",
      "\n",
      "Epoch 21/124\n",
      "train Loss: 0.0470 Acc: 0.9530\n",
      "val Loss: 0.0328 Acc: 0.9672\n",
      "\n",
      "Epoch 22/124\n",
      "train Loss: 0.0461 Acc: 0.9539\n",
      "val Loss: 0.0321 Acc: 0.9679\n",
      "\n",
      "Epoch 23/124\n",
      "train Loss: 0.0463 Acc: 0.9537\n",
      "val Loss: 0.0321 Acc: 0.9679\n",
      "\n",
      "Epoch 24/124\n",
      "train Loss: 0.0471 Acc: 0.9529\n",
      "val Loss: 0.0343 Acc: 0.9657\n",
      "\n",
      "Epoch 25/124\n",
      "train Loss: 0.0418 Acc: 0.9582\n",
      "val Loss: 0.0332 Acc: 0.9668\n",
      "\n",
      "Epoch 26/124\n",
      "train Loss: 0.0482 Acc: 0.9518\n",
      "val Loss: 0.0328 Acc: 0.9672\n",
      "\n",
      "Epoch 27/124\n",
      "train Loss: 0.0498 Acc: 0.9502\n",
      "val Loss: 0.0344 Acc: 0.9656\n",
      "\n",
      "Epoch 28/124\n",
      "train Loss: 0.0484 Acc: 0.9516\n",
      "val Loss: 0.0318 Acc: 0.9682\n",
      "\n",
      "Epoch 29/124\n",
      "train Loss: 0.0448 Acc: 0.9552\n",
      "val Loss: 0.0370 Acc: 0.9630\n",
      "\n",
      "Epoch 30/124\n",
      "train Loss: 0.0461 Acc: 0.9539\n",
      "val Loss: 0.0321 Acc: 0.9679\n",
      "\n",
      "Epoch 31/124\n",
      "train Loss: 0.0461 Acc: 0.9539\n",
      "val Loss: 0.0318 Acc: 0.9682\n",
      "\n",
      "Epoch 32/124\n",
      "train Loss: 0.0446 Acc: 0.9554\n",
      "val Loss: 0.0330 Acc: 0.9670\n",
      "\n",
      "Epoch 33/124\n",
      "train Loss: 0.0448 Acc: 0.9552\n",
      "val Loss: 0.0333 Acc: 0.9667\n",
      "\n",
      "Epoch 34/124\n",
      "train Loss: 0.0474 Acc: 0.9526\n",
      "val Loss: 0.0333 Acc: 0.9667\n",
      "\n",
      "Epoch 35/124\n",
      "train Loss: 0.0480 Acc: 0.9520\n",
      "val Loss: 0.0468 Acc: 0.9532\n",
      "\n",
      "Epoch 36/124\n",
      "train Loss: 0.0448 Acc: 0.9552\n",
      "val Loss: 0.0317 Acc: 0.9683\n",
      "\n",
      "Epoch 37/124\n",
      "train Loss: 0.0484 Acc: 0.9516\n",
      "val Loss: 0.0529 Acc: 0.9471\n",
      "\n",
      "Epoch 38/124\n",
      "train Loss: 0.0452 Acc: 0.9548\n",
      "val Loss: 0.0317 Acc: 0.9683\n",
      "\n",
      "Epoch 39/124\n",
      "train Loss: 0.0438 Acc: 0.9562\n",
      "val Loss: 0.0319 Acc: 0.9681\n",
      "\n",
      "Epoch 40/124\n",
      "train Loss: 0.0485 Acc: 0.9515\n",
      "val Loss: 0.0414 Acc: 0.9586\n",
      "\n",
      "Epoch 41/124\n",
      "train Loss: 0.0455 Acc: 0.9545\n",
      "val Loss: 0.0327 Acc: 0.9673\n",
      "\n",
      "Epoch 42/124\n",
      "train Loss: 0.0453 Acc: 0.9547\n",
      "val Loss: 0.0311 Acc: 0.9689\n",
      "\n",
      "Epoch 43/124\n",
      "train Loss: 0.0438 Acc: 0.9562\n",
      "val Loss: 0.0319 Acc: 0.9681\n",
      "\n",
      "Epoch 44/124\n",
      "train Loss: 0.0409 Acc: 0.9591\n",
      "val Loss: 0.0316 Acc: 0.9684\n",
      "\n",
      "Epoch 45/124\n",
      "train Loss: 0.0478 Acc: 0.9522\n",
      "val Loss: 0.0508 Acc: 0.9492\n",
      "\n",
      "Epoch 46/124\n",
      "train Loss: 0.0457 Acc: 0.9543\n",
      "val Loss: 0.0341 Acc: 0.9659\n",
      "\n",
      "Epoch 47/124\n",
      "train Loss: 0.0466 Acc: 0.9534\n",
      "val Loss: 0.0360 Acc: 0.9640\n",
      "\n",
      "Epoch 48/124\n",
      "train Loss: 0.0437 Acc: 0.9563\n",
      "val Loss: 0.0357 Acc: 0.9643\n",
      "\n",
      "Epoch 49/124\n",
      "train Loss: 0.0466 Acc: 0.9534\n",
      "val Loss: 0.0321 Acc: 0.9679\n",
      "\n",
      "Epoch 50/124\n",
      "train Loss: 0.0453 Acc: 0.9547\n",
      "val Loss: 0.0325 Acc: 0.9675\n",
      "\n",
      "Epoch 51/124\n",
      "train Loss: 0.0477 Acc: 0.9523\n",
      "val Loss: 0.0358 Acc: 0.9642\n",
      "\n",
      "Epoch 52/124\n",
      "train Loss: 0.0459 Acc: 0.9541\n",
      "val Loss: 0.0357 Acc: 0.9643\n",
      "\n",
      "Epoch 53/124\n",
      "train Loss: 0.0433 Acc: 0.9567\n",
      "val Loss: 0.0333 Acc: 0.9667\n",
      "\n",
      "Epoch 54/124\n",
      "train Loss: 0.0473 Acc: 0.9527\n",
      "val Loss: 0.0397 Acc: 0.9603\n",
      "\n",
      "Epoch 55/124\n",
      "train Loss: 0.0466 Acc: 0.9534\n",
      "val Loss: 0.0383 Acc: 0.9617\n",
      "\n",
      "Epoch 56/124\n",
      "train Loss: 0.0464 Acc: 0.9536\n",
      "val Loss: 0.0904 Acc: 0.9096\n",
      "\n",
      "Epoch 57/124\n",
      "train Loss: 0.0456 Acc: 0.9544\n",
      "val Loss: 0.0335 Acc: 0.9665\n",
      "\n",
      "Epoch 58/124\n",
      "train Loss: 0.0482 Acc: 0.9518\n",
      "val Loss: 0.0328 Acc: 0.9672\n",
      "\n",
      "Epoch 59/124\n",
      "train Loss: 0.0447 Acc: 0.9553\n",
      "val Loss: 0.0318 Acc: 0.9682\n",
      "\n",
      "Epoch 60/124\n",
      "train Loss: 0.0426 Acc: 0.9574\n",
      "val Loss: 0.0325 Acc: 0.9675\n",
      "\n",
      "Epoch 61/124\n",
      "train Loss: 0.0511 Acc: 0.9489\n",
      "val Loss: 0.0403 Acc: 0.9597\n",
      "\n",
      "Epoch 62/124\n",
      "train Loss: 0.0421 Acc: 0.9579\n",
      "val Loss: 0.0330 Acc: 0.9670\n",
      "\n",
      "Epoch 63/124\n",
      "train Loss: 0.0481 Acc: 0.9519\n",
      "val Loss: 0.0323 Acc: 0.9677\n",
      "\n",
      "Epoch 64/124\n",
      "train Loss: 0.0462 Acc: 0.9538\n",
      "val Loss: 0.0340 Acc: 0.9660\n",
      "\n",
      "Epoch 65/124\n",
      "train Loss: 0.0449 Acc: 0.9551\n",
      "val Loss: 0.0325 Acc: 0.9675\n",
      "\n",
      "Epoch 66/124\n",
      "train Loss: 0.0446 Acc: 0.9554\n",
      "val Loss: 0.0324 Acc: 0.9676\n",
      "\n",
      "Epoch 67/124\n",
      "train Loss: 0.0464 Acc: 0.9536\n",
      "val Loss: 0.0328 Acc: 0.9672\n",
      "\n",
      "Epoch 68/124\n",
      "train Loss: 0.0452 Acc: 0.9548\n",
      "val Loss: 0.0324 Acc: 0.9676\n",
      "\n",
      "Epoch 69/124\n",
      "train Loss: 0.0472 Acc: 0.9528\n",
      "val Loss: 0.0578 Acc: 0.9422\n",
      "\n",
      "Epoch 70/124\n",
      "train Loss: 0.0443 Acc: 0.9557\n",
      "val Loss: 0.0337 Acc: 0.9663\n",
      "\n",
      "Epoch 71/124\n",
      "train Loss: 0.0432 Acc: 0.9568\n",
      "val Loss: 0.0320 Acc: 0.9680\n",
      "\n",
      "Epoch 72/124\n",
      "train Loss: 0.0502 Acc: 0.9498\n",
      "val Loss: 0.0330 Acc: 0.9670\n",
      "\n",
      "Epoch 73/124\n",
      "train Loss: 0.0444 Acc: 0.9556\n",
      "val Loss: 0.0319 Acc: 0.9681\n",
      "\n",
      "Epoch 74/124\n",
      "train Loss: 0.0487 Acc: 0.9513\n",
      "val Loss: 0.0314 Acc: 0.9686\n",
      "\n",
      "Epoch 75/124\n",
      "train Loss: 0.0493 Acc: 0.9507\n",
      "val Loss: 0.0366 Acc: 0.9634\n",
      "\n",
      "Epoch 76/124\n",
      "train Loss: 0.0447 Acc: 0.9553\n",
      "val Loss: 0.0328 Acc: 0.9672\n",
      "\n",
      "Epoch 77/124\n",
      "train Loss: 0.0452 Acc: 0.9548\n",
      "val Loss: 0.0360 Acc: 0.9640\n",
      "\n",
      "Epoch 78/124\n",
      "train Loss: 0.0410 Acc: 0.9590\n",
      "val Loss: 0.0313 Acc: 0.9687\n",
      "\n",
      "Epoch 79/124\n",
      "train Loss: 0.0486 Acc: 0.9514\n",
      "val Loss: 0.0310 Acc: 0.9690\n",
      "\n",
      "Epoch 80/124\n",
      "train Loss: 0.0433 Acc: 0.9567\n",
      "val Loss: 0.0443 Acc: 0.9557\n",
      "\n",
      "Epoch 81/124\n",
      "train Loss: 0.0497 Acc: 0.9503\n",
      "val Loss: 0.0336 Acc: 0.9664\n",
      "\n",
      "Epoch 82/124\n",
      "train Loss: 0.0448 Acc: 0.9552\n",
      "val Loss: 0.0431 Acc: 0.9569\n",
      "\n",
      "Epoch 83/124\n",
      "train Loss: 0.0488 Acc: 0.9512\n",
      "val Loss: 0.0326 Acc: 0.9674\n",
      "\n",
      "Epoch 84/124\n",
      "train Loss: 0.0443 Acc: 0.9557\n",
      "val Loss: 0.0546 Acc: 0.9454\n",
      "\n",
      "Epoch 85/124\n",
      "train Loss: 0.0447 Acc: 0.9553\n",
      "val Loss: 0.0308 Acc: 0.9692\n",
      "\n",
      "Epoch 86/124\n",
      "train Loss: 0.0477 Acc: 0.9523\n",
      "val Loss: 0.0336 Acc: 0.9664\n",
      "\n",
      "Epoch 87/124\n",
      "train Loss: 0.0447 Acc: 0.9553\n",
      "val Loss: 0.0324 Acc: 0.9676\n",
      "\n",
      "Epoch 88/124\n",
      "train Loss: 0.0440 Acc: 0.9560\n",
      "val Loss: 0.0344 Acc: 0.9656\n",
      "\n",
      "Epoch 89/124\n",
      "train Loss: 0.0451 Acc: 0.9549\n",
      "val Loss: 0.0433 Acc: 0.9567\n",
      "\n",
      "Epoch 90/124\n",
      "train Loss: 0.0466 Acc: 0.9534\n",
      "val Loss: 0.0323 Acc: 0.9677\n",
      "\n",
      "Epoch 91/124\n",
      "train Loss: 0.0454 Acc: 0.9546\n",
      "val Loss: 0.0359 Acc: 0.9641\n",
      "\n",
      "Epoch 92/124\n",
      "train Loss: 0.0473 Acc: 0.9527\n",
      "val Loss: 0.0318 Acc: 0.9682\n",
      "\n",
      "Epoch 93/124\n",
      "train Loss: 0.0472 Acc: 0.9528\n",
      "val Loss: 0.0380 Acc: 0.9620\n",
      "\n",
      "Epoch 94/124\n",
      "train Loss: 0.0412 Acc: 0.9588\n",
      "val Loss: 0.0320 Acc: 0.9680\n",
      "\n",
      "Epoch 95/124\n",
      "train Loss: 0.0482 Acc: 0.9518\n",
      "val Loss: 0.0344 Acc: 0.9656\n",
      "\n",
      "Epoch 96/124\n",
      "train Loss: 0.0447 Acc: 0.9553\n",
      "val Loss: 0.0327 Acc: 0.9673\n",
      "\n",
      "Epoch 97/124\n",
      "train Loss: 0.0443 Acc: 0.9557\n",
      "val Loss: 0.0392 Acc: 0.9608\n",
      "\n",
      "Epoch 98/124\n",
      "train Loss: 0.0473 Acc: 0.9527\n",
      "val Loss: 0.0384 Acc: 0.9616\n",
      "\n",
      "Epoch 99/124\n",
      "train Loss: 0.0493 Acc: 0.9507\n",
      "val Loss: 0.0397 Acc: 0.9603\n",
      "\n",
      "Epoch 100/124\n",
      "train Loss: 0.0437 Acc: 0.9563\n",
      "val Loss: 0.0344 Acc: 0.9656\n",
      "\n",
      "Epoch 101/124\n",
      "train Loss: 0.0489 Acc: 0.9511\n",
      "val Loss: 0.0335 Acc: 0.9665\n",
      "\n",
      "Epoch 102/124\n",
      "train Loss: 0.0434 Acc: 0.9566\n",
      "val Loss: 0.0316 Acc: 0.9684\n",
      "\n",
      "Epoch 103/124\n",
      "train Loss: 0.0432 Acc: 0.9568\n",
      "val Loss: 0.0327 Acc: 0.9673\n",
      "\n",
      "Epoch 104/124\n",
      "train Loss: 0.0431 Acc: 0.9569\n",
      "val Loss: 0.0325 Acc: 0.9675\n",
      "\n",
      "Epoch 105/124\n",
      "train Loss: 0.0451 Acc: 0.9549\n",
      "val Loss: 0.0323 Acc: 0.9677\n",
      "\n",
      "Epoch 106/124\n",
      "train Loss: 0.0466 Acc: 0.9534\n",
      "val Loss: 0.0339 Acc: 0.9661\n",
      "\n",
      "Epoch 107/124\n",
      "train Loss: 0.0459 Acc: 0.9541\n",
      "val Loss: 0.0310 Acc: 0.9690\n",
      "\n",
      "Epoch 108/124\n",
      "train Loss: 0.0440 Acc: 0.9560\n",
      "val Loss: 0.0325 Acc: 0.9675\n",
      "\n",
      "Epoch 109/124\n",
      "train Loss: 0.0414 Acc: 0.9586\n",
      "val Loss: 0.0323 Acc: 0.9677\n",
      "\n",
      "Epoch 110/124\n",
      "train Loss: 0.0446 Acc: 0.9554\n",
      "val Loss: 0.0330 Acc: 0.9670\n",
      "\n",
      "Epoch 111/124\n",
      "train Loss: 0.0479 Acc: 0.9521\n",
      "val Loss: 0.0340 Acc: 0.9660\n",
      "\n",
      "Epoch 112/124\n",
      "train Loss: 0.0412 Acc: 0.9588\n",
      "val Loss: 0.0316 Acc: 0.9684\n",
      "\n",
      "Epoch 113/124\n",
      "train Loss: 0.0521 Acc: 0.9479\n",
      "val Loss: 0.0316 Acc: 0.9684\n",
      "\n",
      "Epoch 114/124\n",
      "train Loss: 0.0469 Acc: 0.9531\n",
      "val Loss: 0.0324 Acc: 0.9676\n",
      "\n",
      "Epoch 115/124\n",
      "train Loss: 0.0455 Acc: 0.9545\n",
      "val Loss: 0.0382 Acc: 0.9618\n",
      "\n",
      "Epoch 116/124\n",
      "train Loss: 0.0424 Acc: 0.9576\n",
      "val Loss: 0.0354 Acc: 0.9646\n",
      "\n",
      "Epoch 117/124\n",
      "train Loss: 0.0457 Acc: 0.9543\n",
      "val Loss: 0.0384 Acc: 0.9616\n",
      "\n",
      "Epoch 118/124\n",
      "train Loss: 0.0469 Acc: 0.9531\n",
      "val Loss: 0.0359 Acc: 0.9641\n",
      "\n",
      "Epoch 119/124\n",
      "train Loss: 0.0470 Acc: 0.9530\n",
      "val Loss: 0.0325 Acc: 0.9675\n",
      "\n",
      "Epoch 120/124\n",
      "train Loss: 0.0443 Acc: 0.9557\n",
      "val Loss: 0.0320 Acc: 0.9680\n",
      "\n",
      "Epoch 121/124\n",
      "train Loss: 0.0442 Acc: 0.9558\n",
      "val Loss: 0.0390 Acc: 0.9610\n",
      "\n",
      "Epoch 122/124\n",
      "train Loss: 0.0460 Acc: 0.9540\n",
      "val Loss: 0.0333 Acc: 0.9667\n",
      "\n",
      "Epoch 123/124\n",
      "train Loss: 0.0456 Acc: 0.9544\n",
      "val Loss: 0.0326 Acc: 0.9674\n",
      "\n",
      "Epoch 124/124\n",
      "train Loss: 0.0444 Acc: 0.9556\n",
      "val Loss: 0.0324 Acc: 0.9676\n",
      "\n",
      "Training complete in 74m 50s\n",
      "Best val Acc: 0.969155\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "#optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#print((dataloaders['train'])[0])\n",
    "model_ft = train_model(model=model_ft, criterion=customLossFunction, optimizer=optimizer_ft, scheduler=exp_lr_scheduler,\n",
    "                       num_epochs=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'FirstModelSaved.pht')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet as fixed feature extractor\n",
    "----------------------------------\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need\n",
    "to set ``requires_grad = False`` to freeze the parameters so that the\n",
    "gradients are not computed in ``backward()``.\n",
    "\n",
    "You can read more about this in the documentation\n",
    "`here <https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don't need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Learning\n",
    "-----------------\n",
    "\n",
    "If you would like to learn more about the applications of transfer learning,\n",
    "checkout our `Quantized Transfer Learning for Computer Vision Tutorial <https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html>`_.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
