{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.epochs = 20\n",
    "        self.cuda=True\n",
    "        self.num_classes = 1\n",
    "        self.batch_size = 4\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_size=0.8\n",
    "        self.dataset = \"InterferometerPhoto\"\n",
    "        self.pin_memory = True\n",
    "        self.momentum = 0.9\n",
    "        self.step_size = 7\n",
    "        self.gamma = 0.1\n",
    "        self.dataset_metadata = \"../data/raw/1channel/reference/epsilon.csv\" \n",
    "        self.num_workers = 0\n",
    "        self.data_root_dir = \"../data/raw/1channel/photo\"\n",
    "        self.data_transforms = transforms.Compose([\n",
    "                #transforms.CenterCrop(448),\n",
    "                #transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.491, 0.491, 0.491],\n",
    "                                      std=[0.210, 0.210, 0.210]) \n",
    "            ])\n",
    "\n",
    "class EpsilonDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(annotation_file,skiprows=0, delim_whitespace=' ')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = np.array(cv2.imread(os.path.join(self.root_dir, str(str(\"%05d\" %self.annotations.imgnr[index]))+ \".png\"))).astype(np.float32)\n",
    "        img=PIL.Image.fromarray(np.uint8(img))\n",
    "        y_label = self.annotations.eps[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, y_label\n",
    "\n",
    "def prepare_data(config):\n",
    "    dataset = EpsilonDataset(config.data_root_dir, config.dataset_metadata, transform=config.data_transforms)\n",
    "    \n",
    "    g = torch.Generator(device=device).manual_seed(0)\n",
    "    train_size = int(config.train_size * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size], generator=g)\n",
    "\n",
    "    print(\"len(train_dataset):\", len(train_dataset),\"len(val_dataset):\", len(val_dataset))\n",
    "\n",
    "    loader_params = dict(batch_size=config.batch_size, num_workers=config.num_workers,\n",
    "                         pin_memory=config.pin_memory, generator=g, shuffle=True)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(**loader_params, dataset=train_dataset )\n",
    "    validation_loader = torch.utils.data.DataLoader(**loader_params, dataset=val_dataset )\n",
    "    \n",
    "    return {'train': train_loader, 'val': validation_loader}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=Config()\n",
    "\n",
    "dataloaders = prepare_data(config)\n",
    "dataset_sizes = {x: len(dataloaders[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataset = EpsilonDataset(config.data_root_dir, config.dataset_metadata, transform=config.data_transforms)\n",
    "\n",
    "train_features, train_labels=next(iter(dataloaders[\"train\"]))\n",
    "\n",
    "print(\"Device: \", device)\n",
    "print(\"Dataloader train len: \", len(dataloaders[\"train\"]), \"val len: \", len(dataloaders[\"val\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLossFunctionDebug(outputs, labels, totalLoss):\n",
    "    print(\"NextOne\")\n",
    "    for i in range (len(outputs)):\n",
    "        print(\"i: \", i, \"label: \", float(labels[i]), \"output:\", float(outputs[i]), \"diff= \", float(min( abs(abs(labels[i])-abs(outputs[i])) , abs(1-(abs(labels[i])-abs(outputs[i]))) )))\n",
    "\n",
    "    print(\"totalLoss:\", float(totalLoss))\n",
    "    return totalLoss\n",
    "\n",
    "\n",
    "def customLossFunction(outputs, labels):\n",
    "    totalLoss=0.0\n",
    "    for i in range (len(outputs)):\n",
    "        oneOutputLoss=min( abs(abs(labels[i])-abs(outputs[i])) , abs(1-(abs(labels[i])-abs(outputs[i]))) )\n",
    "        totalLoss+=oneOutputLoss\n",
    "    totalLoss/=len(outputs)\n",
    "    #customLossFunctionDebug(outputs=outputs, labels=labels, totalLoss=totalLoss)\n",
    "    return totalLoss\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = -100000\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss+=loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            #print(\"epoch_loss: \", epoch_loss, \"running_loss: \", running_loss, \"dataset_sizes[phase]: \", dataset_sizes[phase])\n",
    "            epoch_acc = 1-epoch_loss\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                str(phase), float(epoch_loss), float(epoch_acc)))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(float(best_acc)))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "#optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=config.learning_rate, momentum=config.momentum)\n",
    "\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=config.step_size, gamma=config.gamma)\n",
    "#print((dataloaders['train'])[0])\n",
    "model_ft = train_model(model=model_ft, criterion=customLossFunction, optimizer=optimizer_ft, scheduler=exp_lr_scheduler,\n",
    "                       num_epochs=config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'FirstModelSaved.pht')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do przysz≈Çej analizy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)\n",
    "\n",
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
